Перевод: [[Саянкин Алексей Александрович | Саянкин А.А.]]

__TOC__

<br><br><br><br>
= Apache Hadoop YARN – предыстория и обзор =

== Парадигма MapReduce ==

По существу модель вычислений MapReduce состоит, во-первых, из выполняемой параллельно фазы отображения, в которой входные данные разделяются на конечное множество блоков для последующей обработки. Во-вторых, из из фазы свёртки, в которой вывод фазы отображения агрегируется для получения конечного результата. Простая по сути и должным образом ограниченная программная модель приводит к эффективному и легко масштабируемому на тысячи пользовательских узлов программному коду.

Apache Hadoop MapReduce наиболее популярная реализация модели MapReduce с открытым программным кодом.

В частности, если модель MapReduce используется в паре с распределённой файловой системой Apache Hadoop HDFS,  предоставляющей высокую пропускную способность операций ввода/вывода в больших кластерах, то мы получаем исключительно экономичную и в тоже время весьма производительную систему — ключевой фактор в популярности Hadoop.
Один из принципов работы — это уменьшение перемещения данных между узлами кластера.    То есть мы перемещаем вычисления к данным, а не данные по сети к вычислениям. А именно,  задачи MapReduce могут быть запущены на том физическом узле, который хранит данные в HDFS, при этом задействовав информацию о топологии кластера. Это значительно уменьшает нагрузку на сетевой ввод/вывод и проводит к тому, что весь ввод/вывод осуществляется в пределах локального диска либо одного вычислительно сегмента — важнейшее преимущество.
<br><br><br><br>

== Платформа Apache Hadoop MapReduce ==

Apache Hadoop MapReduce — это проект с открытым исходным кодом копании Apache Software Foundation, представляющий собой реализацию модели вычислений MapReduce, описанную выше. Сам проект Apache Hadoop MapReduce можно разбить на несколько основных частей:
* Программный интерфейс MapReduce API, предназначенный для конечного пользователя, разрабатывающего приложения MapReduce.
* Платформа MapReduce, представляющая собой реализацию времени выполнения различных фаз модели вычислений MapReduce: фазы отображения, сортировки/тасовки/слияния/агрегирования и фазы свёртки. 
* Система MapReduce, которая представляет собой набор библиотек для запуска приложений MapReduce, управления ресурсами кластера, управления выполнением тысяч распределённых параллельных заданий.

Такое распределение ответственности имеет значительные преимущества, в особенности для конечных пользователей — они могут полностью сосредоточиться на разработке приложения с использованием программного интерфейса MapReduce, поручив платформе MapReduce и системе MapReduce управление такими низкоуровневыми процессами как распределение ресурсов, обеспечение отказоустойчивости, планировка заданий. 

В данный момент система Apache Hadoop MapReduce состоит из трекера заданий (JobTracker) — главного процесса и трекеров задач (TaskTrackers) — подчинённых ему процессов.

[[File:MapReduceClassicSystem.png]]

Трекер заданий (JobTracker) отвечает за управление ресурсами (управление рабочими узлами, например, трекерами задач (TaskTrackers), которые там работают), за отслеживание потребления/доступности ресурсов и также за жизненный цикл задания (запуск отдельных задач задания, отслеживание прогресса выполнения задач, обеспечение отказоустойчивости задач)

Трекер задач (TaskTrackers)  имеет простые обязанности — запуск/останов задач по команде трекера заданий (JobTracker) и  переодическое предоставление трекеру заданий информации о статусе задачи. 

Через какое-то время мы поняли, что платформа Apache Hadoop MapReduce требует капитальной переделки. В частности,  используя трекер заданий, нам необходимо было реализовать несколько функций, включая масштабируемость, утилизацию (освобождению ресурсов) кластера , управление обновлениями. Под обновлением мы имеем в виду как обновление пользовательского программного обеспечения, работающего на кластере, так и саму платформу MapReduce.

С течением времени мы вносили новые изменения, например реализация высокой доступности трекера заданий (способность трекера заданий восстановить себя после сбоя), но это привело к более ресурсоёмкой поддержке трекера заданий и не решило главных задач: поддержка модели вычислений, отличной от MapReduce и  обновление пользовательского программного обеспечения.
<br><br><br><br>

== Зачем поддерживать модели вычислений, отличные от MapReduce? ==

Модель вычислений MapReduce прекрасно подходит для многих приложений, но не для всех: другие программные модели, лучше удовлетворяют требованиям, возникающим при обработке графов (Google Pregel / Apache Giraph) и интерактивном моделировании (MPI). Если данные уже загружены в HDFS, то исключительно важно иметь несколько путей для их обработки. 

Более того, поскольку MapReduce по своей сути пакетно-ориентировання модель вычислений, то поддержка обработки данных в реальном или практически реальном времени (например, потоковые вычисления и CEPFresil) — это те задачи, которые нам предстоит решить в ближайшем будущем.
<br><br><br><br>

== Зачем улучшать масштабируемость? ==

Вспомним закон Мура (эмпирическое наблюдение, изначально сделанное Гордоном Муром, согласно которому количество транзисторов, размещаемых на кристалле интегральной схемы, удваивается каждые 24 месяца — ''прим. перев.''). По аналогии вычислительная мощность компьютеров, доступных в дата-центрах, за фиксированную стоимость продолжает быстро расти. Например, сравним типичную мощность узла кластера за разные годы:
* 2009 год — 8 ядер, 16GB памяти RAM, 4x1TB дискового пространства.
* 2012 год — 16+ ядер, 48-96GB памяти RAM, 12x2TB или 12x3TB дискового пространства.

В общем при той же цене, серверы стали в два раза мощнее, чем они были два-три года назад — по каждому из параметров. Apache Hadoop MapReduce успешно управлял кластером из порядка 5000 узлов с аппаратным обеспечением 2009 года выпуска. Следовательно возможность масштабирования должна отражать существующие тенденции в росте аппаратного обеспечения узлов кластера.
<br><br><br><br>

== Какие общие сценарии освобождения ресурсов кластера? ==

В текущей системе (Apache Hadoop MapReduce версии 1 — ''прим. перев.'') трекер заданий рассматривает кластер как набор узлов с чётко заданным описанием слотов отображений и слотов свёрток, которые не являются взаимозаменяемыми. Проблемы с освобождением ресурсов возникают когда слоты отображений могут быть «заполненными», в то время как слоты свёрток пусты (и наоборот). Исправление подобной ситуации необходимо для того  чтобы гарантировать, что вся система в целом использует максимум своей мощности.
<br><br><br><br>

== В чём смысл гибкости платформы для заказчиков? ==

В промышленном использовании Hadoop часто устанавливается как распределённая многопользовательская система. Как результат, изменения и обновления в программном обеспечении Hadoop затрагивают большинство компонент, если не все. С другой стороны пользователи очень болезненно реагируют на необходимость изменения своего кода в связи с изменениями в Hadoop. Поэтому для Hadoop очень важно сделать возможным одновременную работу нескольких версий платформы MapReduce.
<br><br><br><br>

= Введение в Apache Hadoop YARN = 

Главная идея YARN — предоставить две основные функции трекера задний — управление ресурсами и запуск/мониторинг задач — двум отдельным компонентам:  глобальному ''менеджеру ресурсов'' (ResourceManager) и ''мастеру приложений'' (ApplicationMaster, AM). Причём каждое приложение пользователя имеет отдельный экземпляр мастера приложений.
Менеджер ресурсов имеет подчинённый процесс, который называется ''менеджер узлов'' (NodeManager, NM). Менеджер узлов работает отдельно на каждом узле и представляет собой часть общей системы по управлению приложениями в распределённом режиме. 

Менеджер ресурсов является последней инстанцией, решающей вопросы распределения ресурсов между всеми  приложениями системы. Мастер приложений, запускаемый для каждого приложения в отдельности, — это,  в сущности, платформенно зависимый элемент, запрашивающий ресурсы у менеджера ресурсов и взаимодействующий с менеджерами узлов для выполнения и мониторинга задач.

Менеджер ресурсов имеет подключаемый ''планировщик'', который отвечает за выделение ресурсов  разнообразным работающим приложениям. Планировщик представляет собой только планировщик и ничего более в том смысле, что он не выполняет мониторинга и не отслеживает статус приложения, не давая ни каких гарантий по повторному запуску аварийно остановленных задач, вне зависимости от причины останова — программного исключения или аппаратного сбоя. Планировщик выполняет свою работу основываясь на запросах приложения о необходимых ресурсах; Он использует абстрактное понятие ''контейнер ресурсов'', который включает в себя такие элементы как помять, процессор, диск, сеть и другие. 

''Менеджер узлов'' — это подчинённый процесс, работающий на каждом узле и отвечающий за запуск контейнеров приложений, мониторинг использования ресурсов контейнера (процессор, память, диск, сеть) и передачу этих данных менеджеру ресурсов. 
Мастер приложений, запускаемый для каждого приложения в отдельности, отвечает за получение соответствующего контейнера от планировщика, отслеживание статуса контейнера и его прогресса использования. С точки зрения системы, мастер приложений работает как самый обыкновенный ''контейнер''. 

Ниже представлена архитектура YARN.

[[File: MapReduceYARNSystem.png]]

Одна из ключевых деталей реализации MapReduce в новой системе YARN состоит в том, что мы использовали повторно существующую платформу MapReduce без существенного вмешательства в исходный код платформы. Нам было очень важно гарантировать совместимость кода для существующий MapReduce приложений и пользователей.
<br><br><br><br>

= Перечень использованных ссылок =

# http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/


[[Категория:Переводы на русский язык]]

