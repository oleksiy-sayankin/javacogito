Перевод: [[Саянкин Алексей Александрович | Саянкин А.А.]]

__TOC__

<br><br><br><br>
= Apache Hadoop YARN – предыстория и обзор =

== Парадигма MapReduce ==

По существу модель вычислений MapReduce состоит, во-первых, из выполняемой параллельно фазы отображения, в которой входные данные разделяются на конечное множество блоков для последующей обработки. Во-вторых, из из фазы свёртки, в которой вывод фазы отображения агрегируется для получения конечного результата. Простая по сути и должным образом ограниченная программная модель приводит к эффективному и легко масштабируемому на тысячи пользовательских узлов программному коду.

Apache Hadoop MapReduce наиболее популярная реализация модели MapReduce с открытым программным кодом.

В частности, если модель MapReduce используется в паре с распределённой файловой системой Apache Hadoop HDFS,  предоставляющей высокую пропускную способность операций ввода/вывода в больших кластерах, то мы получаем исключительно экономичную и в тоже время весьма производительную систему — ключевой фактор в популярности Hadoop.
Один из принципов работы — это уменьшение перемещения данных между узлами кластера.    То есть мы перемещаем вычисления к данным, а не данные по сети к вычислениям. А именно,  задачи MapReduce могут быть запущены на том физическом узле, который хранит данные в HDFS, при этом задействовав информацию о топологии кластера. Это значительно уменьшает нагрузку на сетевой ввод/вывод и проводит к тому, что весь ввод/вывод осуществляется в пределах локального диска либо одного вычислительно сегмента — важнейшее преимущество.
<br><br><br><br>

== Платформа Apache Hadoop MapReduce ==

Apache Hadoop MapReduce — это проект с открытым исходным кодом копании Apache Software Foundation, представляющий собой реализацию модели вычислений MapReduce, описанную выше. Сам проект Apache Hadoop MapReduce можно разбить на несколько основных частей:
* Программный интерфейс MapReduce API, предназначенный для конечного пользователя, разрабатывающего приложения MapReduce.
* Платформа MapReduce, представляющая собой реализацию времени выполнения различных фаз модели вычислений MapReduce: фазы отображения, сортировки/тасовки/слияния/агрегирования и фазы свёртки. 
* Система MapReduce, которая представляет собой набор библиотек для запуска приложений MapReduce, управления ресурсами кластера, управления выполнением тысяч распределённых параллельных заданий.

Такое распределение ответственности имеет значительные преимущества, в особенности для конечных пользователей — они могут полностью сосредоточиться на разработке приложения с использованием программного интерфейса MapReduce, поручив платформе MapReduce и системе MapReduce управление такими низкоуровневыми процессами как распределение ресурсов, обеспечение отказоустойчивости, планировка заданий. 

В данный момент система Apache Hadoop MapReduce состоит из трекера заданий (JobTracker) — главного процесса и трекеров задач (TaskTrackers) — подчинённых ему процессов.

[[File:MapReduceClassicSystem.png | Схема кластера с классической реализацией Apache Hadoop MapReduce]]

Трекер заданий (JobTracker) отвечает за управление ресурсами (управление рабочими узлами, например, трекерами задач (TaskTrackers), которые там работают), за отслеживание потребления/доступности ресурсов и также за жизненный цикл задания (запуск отдельных задач задания, отслеживание прогресса выполнения задач, обеспечение отказоустойчивости задач)

Трекер задач (TaskTrackers)  имеет простые обязанности — запуск/останов задач по команде трекера заданий (JobTracker) и  переодическое предоставление трекеру заданий информации о статусе задачи. 

Через какое-то время мы поняли, что платформа Apache Hadoop MapReduce требует капитальной переделки. В частности,  используя трекер заданий, нам необходимо было реализовать несколько функций, включая масштабируемость, утилизацию (освобождению ресурсов) кластера , управление обновлениями. Под обновлением мы имеем в виду как обновление пользовательского программного обеспечения, работающего на кластере, так и саму платформу MapReduce.

С течением времени мы вносили новые изменения, например реализация высокой доступности трекера заданий (способность трекера заданий восстановить себя после сбоя), но это привело к более ресурсоёмкой поддержке трекера заданий и не решило главных задач: поддержка модели вычислений, отличной от MapReduce и  обновление пользовательского программного обеспечения.
<br><br><br><br>

== Зачем поддерживать модели вычислений, отличные от MapReduce? ==

Модель вычислений MapReduce прекрасно подходит для многих приложений, но не для всех: другие программные модели, лучше удовлетворяют требованиям, возникающим при обработке графов (Google Pregel / Apache Giraph) и интерактивном моделировании (MPI). Если данные уже загружены в HDFS, то исключительно важно иметь несколько путей для их обработки. 

Более того, поскольку MapReduce по своей сути пакетно-ориентировання модель вычислений, то поддержка обработки данных в реальном или практически реальном времени (например, потоковые вычисления и CEPFresil) — это те задачи, которые нам предстоит решить в ближайшем будущем.
<br><br><br><br>

== Зачем улучшать масштабируемость? ==

Вспомним закон Мура (эмпирическое наблюдение, изначально сделанное Гордоном Муром, согласно которому количество транзисторов, размещаемых на кристалле интегральной схемы, удваивается каждые 24 месяца — ''прим. перев.''). По аналогии вычислительная мощность компьютеров, доступных в дата-центрах, за фиксированную стоимость продолжает быстро расти. Например, сравним типичную мощность узла кластера за разные годы:
* 2009 год — 8 ядер, 16GB памяти RAM, 4x1TB дискового пространства.
* 2012 год — 16+ ядер, 48-96GB памяти RAM, 12x2TB или 12x3TB дискового пространства.

В общем при той же цене, серверы стали в два раза мощнее, чем они были два-три года назад — по каждому из параметров. Apache Hadoop MapReduce успешно управлял кластером из порядка 5000 узлов с аппаратным обеспечением 2009 года выпуска. Следовательно возможность масштабирования должна отражать существующие тенденции в росте аппаратного обеспечения узлов кластера.
<br><br><br><br>

== Какие общие сценарии освобождения ресурсов кластера? ==

В текущей системе (Apache Hadoop MapReduce версии 1 — ''прим. перев.'') трекер заданий рассматривает кластер как набор узлов с чётко заданным описанием слотов отображений и слотов свёрток, которые не являются взаимозаменяемыми. Проблемы с освобождением ресурсов возникают когда слоты отображений могут быть «заполненными», в то время как слоты свёрток пусты (и наоборот). Исправление подобной ситуации необходимо для того  чтобы гарантировать, что вся система в целом использует максимум своей мощности.
<br><br><br><br>

== В чём смысл гибкости платформы для заказчиков? ==

В промышленном использовании Hadoop часто устанавливается как распределённая многопользовательская система. Как результат, изменения и обновления в программном обеспечении Hadoop затрагивают большинство компонент, если не все. С другой стороны пользователи очень болезненно реагируют на необходимость изменения своего кода в связи с изменениями в Hadoop. Поэтому для Hadoop очень важно сделать возможным одновременную работу нескольких версий платформы MapReduce.
<br><br><br><br>

== Введение в Apache Hadoop YARN == 

Главная идея YARN — предоставить две основные функции трекера задний — управление ресурсами и запуск/мониторинг задач — двум отдельным компонентам:  глобальному ''менеджеру ресурсов'' (ResourceManager) и ''мастеру приложений'' (ApplicationMaster, AM). Причём каждое приложение пользователя имеет отдельный экземпляр мастера приложений.
Менеджер ресурсов имеет подчинённый процесс, который называется ''менеджер узлов'' (NodeManager, NM). Менеджер узлов работает отдельно на каждом узле и представляет собой часть общей системы по управлению приложениями в распределённом режиме. 

Менеджер ресурсов является последней инстанцией, решающей вопросы распределения ресурсов между всеми  приложениями системы. Мастер приложений, запускаемый для каждого приложения в отдельности, — это,  в сущности, платформенно зависимый элемент, запрашивающий ресурсы у менеджера ресурсов и взаимодействующий с менеджерами узлов для выполнения и мониторинга задач.

Менеджер ресурсов имеет подключаемый ''планировщик'', который отвечает за выделение ресурсов  разнообразным работающим приложениям. Планировщик представляет собой только планировщик и ничего более в том смысле, что он не выполняет мониторинга и не отслеживает статус приложения, не давая ни каких гарантий по повторному запуску аварийно остановленных задач, вне зависимости от причины останова — программного исключения или аппаратного сбоя. Планировщик выполняет свою работу основываясь на запросах приложения о необходимых ресурсах; Он использует абстрактное понятие ''контейнер ресурсов'', который включает в себя такие элементы как помять, процессор, диск, сеть и другие. 

''Менеджер узлов'' — это подчинённый процесс, работающий на каждом узле и отвечающий за запуск контейнеров приложений, мониторинг использования ресурсов контейнера (процессор, память, диск, сеть) и передачу этих данных менеджеру ресурсов. 
Мастер приложений, запускаемый для каждого приложения в отдельности, отвечает за получение соответствующего контейнера от планировщика, отслеживание статуса контейнера и его прогресса использования. С точки зрения системы, мастер приложений работает как самый обыкновенный ''контейнер''. 

Ниже представлена архитектура YARN.

[[File: MapReduceYARNSystem.png | Схема кластера с YARN реализацией Apache Hadoop MapReduce]]

Одна из ключевых деталей реализации MapReduce в новой системе YARN состоит в том, что мы использовали повторно существующую платформу MapReduce без существенного вмешательства в исходный код платформы. Нам было очень важно гарантировать совместимость кода для существующий MapReduce приложений и пользователей.
<br><br><br><br>

= Apache Hadoop YARN — концепции и применение =

Как было сказано выше YARN — это, по существу, система управления распределёнными приложениями. Она состоит из глобального менеджера ресурсов, который управляет всеми доступными ресурсами кластера и, работающих на каждом узле менеджеров узлов, которыми управляет менеджер ресурсов. Менеджер узлов отвечает за распределение ресурсов на на каждом узле. 

Рассмотрим компоненты более подробно.

== Менеджер ресурсов ==

В YARN менеджер ресурсов — это только планировщик и не более. По существу он строго ограничен лишь тем, что распределяет ресурсы системы между конкурирующими за ними приложениями, если хотите он как брокерская фирма, котирующая ценные бумаги. Он оптимизирует высвобождение ресурсов кластера (следит, чтобы не было «простаивающих в холостую» ресурсов) с учётом множества ограничений, таких как обязанность выделить все необходимые ресурсы, которые запрашивает приложение, равнодоступность ресурсов для всех приложений и права доступа к ресурсам. Для реализации различных ограничений менеджер ресурсов имеет подключаемый планировщик, который позволяет при необходимости использовать алгоритмы сбалансированного распределения ресурсов.

== Мастер приложений ==

Многие проводят параллели между YARN и существующей системой Hadoop MapReduce (называемой MR1 в Apache Hadoop 1.x). Однако существует ключевое различие — это новая концепция мастера приложений. 

Мастер приложений, в действительности является экземпляром библиотеки, специфической относительно платформы. Он отвечает за получение ресурсов от менеджера ресурсов.  Мастер приложений взаимодействует с менеджером узлов для выделения и мониторинга контейнеров. В обязанности мастера приложений входит запрос у менеджера ресурсов соответствующего контейнера ресурсов, отслеживание статуса контейнера и мониторинг прогресса выполнения задачи.

Мастер приложений позволяет платформе YARN достичь следующих ключевых характеристик:

* Масштабируемость. По сути мастер приложений представляет большую часть традиционного функционала менеджера ресурсов так что система в целом масштабируется достаточно легко. Мы уже провели тестирование масштабирования кластера размером до 10 000 узлов без каких либо значительных затруднений. Менеджер ресурсов является только лишь планировщиком то есть не предоставляет средств отказоустойчивости для ресурсов. Мы переместили эти задачи в зону ответственности менеджера приложений. Более того, поскольку теперь каждому приложению соответствует экземпляр мастера приложений, то сам по себе мастер перестал быть «бутылочным горлышком» кластера. 
* Открытость. Переместив весь код, специфический для приложения в мастер приложений, мы получаем общую систему, которая может поддерживать одновременно несколько платформ, таких как MapReduce, MPI (Message Passing Interface,  интерфейс передачи сообщений — программный интерфейс (API) для передачи информации, который позволяет обмениваться сообщениями между процессами, выполняющими одну задачу — ''прим. перев.'') и Graph Processing (обработка графов).

Добавим также несколько дополнительных характеристик YARN платформы:

* Мы переместили всю сложность (по мере возможности, конечно) в мастер приложений, предоставив ему всю необходимую функциональность, позволяющую разработчикам достичь гибкости и силы в реализации своего кода.
* Не доверяйте мастеру приложений, поскольку его код в большинстве своём — пользовательский код, то есть мастер приложений не является привилегированным сервисом.
* Система YARN (менеджер узлов и менеджер ресурсов) вынуждены «защищать» сами себя от неисправных или «злонамеренных» мастеров приложений, желающих получить все ресурсы любой ценой.


Полезно помнить, что в реальности каждое приложение имеет свой собственный экземпляр менеджера приложений. Тем не менее, также волне допустимо реализовать мастер приложений, управляющий набором приложений (например, мастер приложений для Pig или Hive, который управляет целым набором задач MapReduce). К тому же, эта концепция была развита для управления долгоживущими сервисами, которые в свою очередь управляют своими собственными приложениями (например, запускать HBase в YARN с помощью HBaseAppMaster). 

== Модель ресурсов ==

YARN поддерживает очень общую модель ресурсов для приложений. Приложение (с помощью мастера приложений) может запрашивать ресурсы предельно точно определяя свои  требования, такие как:

* Имя ресурса
* Память (в Мб)
* Процессор (количество ядер)
* Диск, сетевой ввод/вывод 

== Контейнер и запрос на предоставление ресурсов ==

YARN задумывался так, чтобы позволить отдельным приложениям (через мастер приложений) использовать ресурсы кластера в распределенном многопользовательском окружении. Также важно знать топологию кластера для эффективного планирования и оптимизации доступа к данным, то есть уменьшение до минимума перемещения данных.

Для достижения поставленных целей, главный планировщик (в составе менеджера ресурсов) имеет полную информацию о запросах приложений на ресурсы, что позволяет ему делать распределение ресурсов оптимальным образом между всеми приложениями кластера. Это приводит нас к ''запросам на предоставление ресурсов'' и ''контейнерам''. 

По сути, приложение может выполнить запрос на предоставление необходимых ресурсов посредством мастера приложений. Планировщик отвечает на запрос о предоставлении ресурсов выдачей контейнера, который удовлетворяет требованиям, выставленным мастером приложений в запросе. Давайте рассмотрим запрос на предоставление ресурсов более подробно. Запрос имеет следующую форму:

<имя-ресурса, приоритет, неоходимый-ресурс, число-контейнеров>

Для лучшего понимания рассмотрим каждый компонент запроса на предоставление ресурса более подробно:

* имя-ресурса — это либо имя узла, либо имя сегмента кластера либо *, что означает неопределённое значение. В будущем мы планируем поддерживать более сложные топологии для случая, когда в кластере используются виртуальные машины, либо сложные сетевые топологии.
* приоритет — внутренний приоритет запроса, среди других запросов на предоставление ресурсов этого же приложения. (Обратите внимание, это не приоритет запроса между разными приложениями).
* неоходимый-ресурс — это запрашиваемые мощности, такие как память, процессор. На момент написания данной статьи (Август 2012 — ''прим. перев.'') это только  память и процессор в качестве параметров контейнера.
* число-контейнеров — количество контейнеров с заданными ранее параметрами.

Теперь рассмотрим контейнеры. По сути контейнер — это выделенные ресурсы, результат успешного выполнения менеджером ресурсов определённого запроса на выделение ресурсов. Контейнер предоставляет права приложению для использование заданного количества ресурсов (память, процессор) на ''заданном узле''.

Мастер приложений должен взять контейнер и передать его менеджеру узлов, управляющему  узлом, на котором расположен контейнер для использования ресурсов при запуске пользовательских задач. Конечно, в безопасном режиме работы кластера проверяются привилегии на выделение ресурсов, чтобы гарантировать отсутствие неправомерных запросов на выделение ресурсов.

== Запуск и спецификация контейнера ==

Поскольку контейнер, как описано выше, это всего лишь право использовать заданное количество ресурсов на заданном узле кластера (на котором находится менеджер узлов), то мастер приложений должен предоставить менеджеру узлов более подробную информацию для фактического ''запуска'' контейнера.

YARN позволяет запускать приложения на разных языках и в отличие от существующей версии Hadoop MapReduce  hadoop-1.x (также известной как MR1) не ограничен языком программирования Java.

Программный интерфейс запуска YARN контейнера не зависит от платформы и содержит:

* Интерфейс командой строки для запуска процесса внутри контейнера.
* Переменные окружения.
* Локальные ресурсы, необходимые перед запуском, такие как файлы jar, общие объекты, дополнительные файлы данных.
* Токены безопасности.

Это позволяет мастеру приложений взаимодействовать с менеджером узлов для запуска контейнеров, выполняя широкий спектр приложений, начиная от простых скриптов на C/Java/Python и заканчивая полноценными виртуальными машинами, например KVM (KVM или Kernel-based Virtual Machine - программное решение, обеспечивающее виртуализацию в среде Linux на платформе x86, которая поддерживает аппаратную виртуализацию на базе Intel VT (Virtualization Technology) либо AMD SVM (Secure Virtual Machine) — ''прим. перев.'').

== YARN — обзор и анализ ==

Вооружившись знаниями из предыдущих разделов будет полезно выяснить как собственно приложения работают в YARN. Запуск приложения состоит из следующих шагов:

* Выполнение запроса на запуск приложения.
* Запуск экземпляра мастера приложений для работы с приложением.
* Выполнение приложения под управлением экземпляра мастера приложений.

Пройдёмся по последовательности шагов для выполнения приложения (номера шагов показаны ниже на диаграмме):

# Клиентская программа ''выполняет запрос на запуск'' приложения, содержащий кроме всего прочего необходимые данные для запуска мастера приложений, соответствующего приложению. 
# Менеджер ресурсов принимает на себя ответственность за выделение необходимого контейнера, в котором будет запущен мастер приложений и затем ''запускает'' мастер приложений. 
# В ходе первоначальной загрузки мастер приложений ''регистрируется'' у менеджера ресурсов. Регистрация позволяет клиентской программе запрашивать у менеджера ресурсов специфическую информацию, необходимую для непосредственного взаимодействия с мастером приложений.
# В ходе штатной работы мастер приложений  с помощью протокола запроса ресурсов запрашивает соответствующий контейнер ресурсов.
# После успешно получения контейнера мастер приложений запускает контейнер, предоставляя менеджеру узлов спецификацию запуска контейнера. Спецификация запуска контейнера обычно содержит необходимую информацию, позволяющую контейнеру взаимодействовать с мастером приложений.
# Внутри контейнера происходит старт кода пользовательского приложения. Затем пользовательское приложение посредством специального протокола предоставляет информацию (этап выполнения, статус) своему мастеру приложения.
# В ходе работы пользовательского приложения клиент, выполнивший запрос на запуск приложения, взаимодействует посредством специально протокола непосредственно с мастером приложения для получения статуса и процента выполнения задачи.
# Когда приложение завершило выполнение и вся необходимая работа была выполнена, мастер приложений отменяет  регистрацию у менеджера ресурсов и останавливает своё выполнение, освобождая контейнер для других целей.

[[File: YARNApplicationExecutionSteps.png | Шаги запуска приложений в YARN]]

= Перечень использованных ссылок =

# http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/
# http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/


[[Категория:Переводы на русский язык]]

